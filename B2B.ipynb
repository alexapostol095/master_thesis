{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Library Imports, Data Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.cluster import KMeans\n",
    "import matplotlib.pyplot as plt\n",
    "import statsmodels.api as sm\n",
    "import statsmodels.formula.api as smf\n",
    "import seaborn as sns\n",
    "import itertools\n",
    "from sklearn.metrics import silhouette_score\n",
    "import networkx as nx\n",
    "from collections import defaultdict\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn_extra.cluster import KMedoids\n",
    "from itertools import combinations\n",
    "from statsmodels.stats.anova import anova_lm\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data import section is slightly altered to cover any specific references to the firm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import orderlines\n",
    "OrderLinesData = pd.read_csv('data/Symson Orderlines 20241121.csv', encoding='ISO-8859-1', sep=';', decimal=',')\n",
    "\n",
    "# Import product data\n",
    "Products = pd.read_csv('data/Symson Products 20241121.csv', encoding='ISO-8859-1', sep=';', decimal=',')\n",
    "\n",
    "# Rename columns in Products\n",
    "Products.rename(columns={'ProductAttributeC':'PriceChangeFrequency','ProductAttributeE':'CompetitorIntensity', 'productId':'ProductId',\n",
    "                         'ProductAttributeF':'ProductType','ProductAttributeB':'Availability','ProductAttributeG':'Sustainability'}, inplace=True)\n",
    "\n",
    "# Rename deals\n",
    "OrderLinesData.rename(columns={'OrderLineAttributeD':'Deals'}, inplace=True)\n",
    "\n",
    "# New OrderLines\n",
    "NewOrderLines = pd.read_csv('data/Orderlines.csv', encoding='ISO-8859-1', sep=';', decimal=',')\n",
    "\n",
    "# Rename orderline attributes ProjectDiscount and Matrix\n",
    "NewOrderLines.rename(columns={'OrderlineAttributeE':'ProjectDiscount','OrderlineAttributeF':'Matrix'}, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Processing & Attribute Creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "OrderLinesData = OrderLinesData[(OrderLinesData['PricePerUnit']>0) & (OrderLinesData['Quantity']>=1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "OrderLinesData = OrderLinesData.drop_duplicates(subset=['InvoiceId', 'ProductId'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Month\n",
    "OrderLinesData['Date Month'] = pd.to_datetime(OrderLinesData['InvoiceCreationDate']).dt.to_period('M')\n",
    "\n",
    "# Availability\n",
    "Products['Availability'] = np.where(Products['Availability']=='ON STOCK                                ', 1, 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regression Preparation and Execution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Quantity = OrderLinesData.groupby([\"ProductId\", \"PricePerUnit\", \"InvoiceCreationDate\"])['Quantity'].sum().reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Quantity\n",
    "Quantity = OrderLinesData.filter([\"ProductId\", \"Quantity\",'Date Month'])\n",
    "Quantity = Quantity.groupby([\"ProductId\",'Date Month']).sum().reset_index()\n",
    "\n",
    "# Price\n",
    "Price = OrderLinesData.filter(['ProductId','Date Month'])\n",
    "Price.drop_duplicates(inplace=True)\n",
    "W_Average_Prices = OrderLinesData.groupby(['ProductId','Date Month']).apply(lambda x: np.sum(x['Quantity']*x['PricePerUnit'])/x['Quantity'].sum(), include_groups=False).reset_index(name='PricePerUnit')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_customers = OrderLinesData.groupby(\"ProductId\")[\"CustomerId\"].nunique().rename(\"NumCustomers\").reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfRegression = pd.merge(Quantity, W_Average_Prices, on=['ProductId','Date Month'], how='left') \n",
    "dfRegression = pd.merge(dfRegression, OrderLinesData[['ProductId','Date Month']].drop_duplicates(), on=['ProductId','Date Month'], how='left')\n",
    "dfRegression = pd.merge(dfRegression, Products.filter(['ProductId','PriceChangeFrequency','ProductType','Availability']), on='ProductId', how='left')\n",
    "dfRegression = pd.merge(dfRegression, num_customers, on='ProductId', how='left')\n",
    "dfRegression.dropna(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(OrderLinesData)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "OrderLinesData[\"ProductId\"].nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfRegression_t = dfRegression.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfRegression_t[\"MainProduct\"] = np.where(dfRegression_t[\"ProductType\"] == \"HOOFDPRODUCT\", 1, 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# KVI Work"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Slightly concealed to hide firm-specific references"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Quantity\n",
    "Quantity = OrderLinesData.filter([\"ProductId\", \"Quantity\"])\n",
    "Quantity = Quantity.groupby([\"ProductId\"]).sum().reset_index()\n",
    "\n",
    "# Price\n",
    "Price = OrderLinesData.filter(['ProductId'])\n",
    "Price.drop_duplicates(inplace=True)\n",
    "W_Average_Prices = OrderLinesData.groupby(['ProductId']).apply(lambda x: np.sum(x['Quantity']*x['PricePerUnit'])/x['Quantity'].sum(), include_groups=False).reset_index(name='PricePerUnit')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfSensitivity = pd.merge(Quantity, W_Average_Prices, on=['ProductId'], how='left') \n",
    "dfSensitivity = pd.merge(dfSensitivity, Products.filter(['ProductId','PriceChangeFrequency','ProductType','Availability']), on='ProductId', how='left')\n",
    "dfSensitivity = pd.merge(dfSensitivity, num_customers, on='ProductId', how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfSensitivity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "KVI_Analysis = pd.read_csv('data/KVI_List.csv')\n",
    "KVI_Analysis = KVI_Analysis[KVI_Analysis[\"Category\"] == \"KVI\"]\n",
    "KVI_list = KVI_Analysis[\"ProductId\"].tolist()\n",
    "dfSensitivity[\"KVI_dummy\"] = dfSensitivity[\"ProductId\"].isin(KVI_list).astype(int)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## KVI Clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfAllStandard = dfSensitivity.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfAllStandard = dfAllStandard.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "product_purchase_count = OrderLinesData[\"ProductId\"].value_counts().reset_index()\n",
    "product_purchase_count.columns = [\"ProductId\", \"PurchaseCount\"]\n",
    "dfAllStandard= dfAllStandard.merge(product_purchase_count, on=\"ProductId\", how=\"left\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfAllStandard[\"ProductType\"] = np.where(dfAllStandard[\"ProductType\"] == \"HOOFDPRODUCT\", 1, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfAllStandard[\"Revenue\"] = dfAllStandard[\"Quantity\"] * dfAllStandard[\"PricePerUnit\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "df_standard_product_ids = dfAllStandard['ProductId'].tolist()\n",
    "basket_groups = OrderLinesData.groupby('InvoiceId')['ProductId'].apply(list)\n",
    "\n",
    "co_purchase_counts = defaultdict(int)\n",
    "product_counts = defaultdict(int)  \n",
    "\n",
    "for basket in basket_groups:\n",
    "    filtered_basket = [prod for prod in basket if prod in df_standard_product_ids]\n",
    "    for i in range(len(filtered_basket)):\n",
    "        product_counts[filtered_basket[i]] += 1  \n",
    "        for j in range(i + 1, len(filtered_basket)):\n",
    "            pair = tuple(sorted([filtered_basket[i], filtered_basket[j]]))  \n",
    "            co_purchase_counts[pair] += 1\n",
    "\n",
    "\n",
    "G = nx.Graph()\n",
    "\n",
    "for (product1, product2), weight in co_purchase_counts.items():\n",
    "    G.add_edge(product1, product2, weight=weight)\n",
    "\n",
    "pos = nx.spring_layout(G, seed=42, k=5) \n",
    "\n",
    "edge_colors = []\n",
    "edge_widths = []\n",
    "\n",
    "max_weight = max(co_purchase_counts.values())\n",
    "\n",
    "for u, v, d in G.edges(data=True):\n",
    "    weight = d[\"weight\"]\n",
    "    edge_widths.append(weight / max_weight * 12)  \n",
    "    \n",
    "\n",
    "plt.figure(figsize=(35, 35))\n",
    "\n",
    "nx.draw_networkx_nodes(G, pos, edgecolors=\"black\")\n",
    "\n",
    "nx.draw_networkx_edges(G, pos, alpha=1, width=edge_widths)\n",
    "\n",
    "G.remove_edges_from(nx.selfloop_edges(G))\n",
    "\n",
    "plt.title(\"Product Co-Purchase Network\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "degree_centrality = nx.degree_centrality(G)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfAllStandard[\"DegreeCentrality\"] = dfAllStandard[\"ProductId\"].map(degree_centrality)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Takes 45 minutes to run, stored in csv for easier access\n",
    "#betweenness_centrality = nx.betweenness_centrality(G, k=None, weight=\"weight\")\n",
    "# betweenness_centrality_df = pd.DataFrame(list(betweenness_centrality.items()), columns=['ProductId', 'BetweennessCentrality'])\n",
    "# betweenness_centrality_df.to_csv('betweenness_centrality.csv', index=False)\n",
    "betweenness_centrality_df = pd.read_csv('data/betweenness_centrality.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfAllStandard = dfAllStandard.merge(betweenness_centrality_df, on=\"ProductId\", how=\"left\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(dfAllStandard)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfAllStandard.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfAllStandard = dfAllStandard.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(dfAllStandard))\n",
    "dfAllStandard = dfAllStandard[dfAllStandard[\"BetweennessCentrality\"] > 0]\n",
    "print(len(dfAllStandard))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sorted = dfAllStandard.sort_values(by='Revenue', ascending=False).reset_index(drop=True)\n",
    "\n",
    "# Calculate cumulative revenue\n",
    "df_sorted['CumulativeRevenue'] = df_sorted['Revenue'].cumsum()\n",
    "df_sorted['CumulativeRevenue'] /= df_sorted['Revenue'].sum()  # Normalize to make it proportion\n",
    "\n",
    "# Calculate x-axis as percentage of products\n",
    "df_sorted['ProductPercent'] = np.linspace(0, 100, len(df_sorted))\n",
    "\n",
    "# Plot\n",
    "plt.figure(figsize=(8, 5))\n",
    "plt.plot(df_sorted['ProductPercent'], df_sorted['CumulativeRevenue']*100, marker=',', linestyle='-')\n",
    "plt.xlabel(\"Top Percentage of Products Sorted by Revenue\")\n",
    "plt.ylabel(\"Cumulative Revenue Share (%)\")\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cut_points = [0.5, 0.8, 0.9, 0.95, 0.96, 0.97, .98]\n",
    "\n",
    "for cp in cut_points:\n",
    "    idx = df_sorted[df_sorted['CumulativeRevenue'] >= cp].index[0]\n",
    "    percent_of_products = df_sorted.loc[idx, 'ProductPercent']\n",
    "    print(f\"{cp*100:.0f}% of revenue is generated by the top {percent_of_products:.2f}% of products.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sorted[df_sorted[\"ProductPercent\"] <= 25]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(df_sorted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfAllStandard = dfAllStandard.sort_values(by='Revenue', ascending=False).head(round(len(dfAllStandard) / 4)-1)\n",
    "print(len(dfAllStandard))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfAllStandard.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfAllStandard.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfAllStandard[[\"Quantity\", \"PricePerUnit\", \"Revenue\", \"NumCustomers\", \n",
    "               \"PurchaseCount\", \"Availability\", \"PriceChangeFrequency\", \n",
    "               \"ProductType\", \"DegreeCentrality\", \"BetweennessCentrality\"]].describe()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfAllStandard[\"NumCustomers\"] = np.log(dfAllStandard[\"NumCustomers\"])\n",
    "dfAllStandard[\"PurchaseCount\"] = np.log(dfAllStandard[\"PurchaseCount\"])\n",
    "dfAllStandard[\"PricePerUnit\"] = np.log(dfAllStandard[\"PricePerUnit\"])\n",
    "dfAllStandard[\"DegreeCentrality\"] = np.log(dfAllStandard[\"DegreeCentrality\"])\n",
    "dfAllStandard[\"BetweennessCentrality\"] = np.log(dfAllStandard[\"BetweennessCentrality\"])\n",
    "dfAllStandard[\"Revenue\"] = np.log(dfAllStandard[\"Revenue\"])\n",
    "dfAllStandard[\"Quantity\"] = np.log(dfAllStandard[\"Quantity\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "features_to_scale = ['PricePerUnit', 'PurchaseCount', 'NumCustomers', \n",
    "                     'DegreeCentrality', 'BetweennessCentrality','Revenue', 'Quantity']\n",
    "\n",
    "scaler = RobustScaler()\n",
    "\n",
    "for feature in features_to_scale:\n",
    "    dfAllStandard[feature + '_Scaled'] = scaler.fit_transform(dfAllStandard[[feature]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfAllStandard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "features = [\n",
    "    'Quantity_Scaled', 'Revenue_Scaled',\n",
    "    'NumCustomers_Scaled', 'PurchaseCount_Scaled', 'PricePerUnit_Scaled'\n",
    "]\n",
    "\n",
    "X = dfAllStandard[features].values  \n",
    "\n",
    "# 1. Apply PCA\n",
    "pca = PCA()\n",
    "X_pca = pca.fit_transform(X)\n",
    "\n",
    "# 2. Explained variance plot (Scree plot)\n",
    "plt.figure(figsize=(8, 5))\n",
    "plt.plot(range(1, len(pca.explained_variance_ratio_) + 1), \n",
    "         np.cumsum(pca.explained_variance_ratio_), \n",
    "         marker='o', linestyle='--')\n",
    "plt.xlabel(\"Number of Principal Components\")\n",
    "plt.ylabel(\"Cumulative Explained Variance\")\n",
    "plt.grid()\n",
    "plt.xticks(range(1, len(pca.explained_variance_ratio_) + 1)) \n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca.explained_variance_ratio_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set number of components for PCA\n",
    "n_components = 3\n",
    "\n",
    "# Reduce dimensionality using PCA\n",
    "pca = PCA(n_components=n_components)\n",
    "X_pca_reduced = pca.fit_transform(X)\n",
    "\n",
    "# 1. Apply K-Means Clustering\n",
    "wcss_kmeans = []  # Within-cluster sum of squares for K-Means\n",
    "silhouette_scores_kmeans = []  # Silhouette scores for K-Means\n",
    "K_range = range(2, 60)  # K-means clustering from 2 to 39 clusters\n",
    "\n",
    "for k in K_range:\n",
    "    kmeans = KMeans(n_clusters=k, random_state=42, n_init=10)\n",
    "    kmeans.fit(X_pca_reduced)\n",
    "    wcss_kmeans.append(kmeans.inertia_)  # WCSS for K-means\n",
    "    \n",
    "    # Compute silhouette score for K-means\n",
    "    silhouette_avg_kmeans = silhouette_score(X_pca_reduced, kmeans.labels_)\n",
    "    silhouette_scores_kmeans.append(silhouette_avg_kmeans)\n",
    "\n",
    "# 2. Apply K-Medoids Clustering\n",
    "wcss_kmedoids = []  # Within-cluster sum of squares for K-Medoids\n",
    "silhouette_scores_kmedoids = []  # Silhouette scores for K-Medoids\n",
    "\n",
    "for k in K_range:\n",
    "    kmedoids = KMedoids(n_clusters=k, random_state=42)\n",
    "    kmedoids.fit(X_pca_reduced)\n",
    "    wcss_kmedoids.append(kmedoids.inertia_)  # WCSS for K-medoids\n",
    "    \n",
    "    # Compute silhouette score for K-medoids\n",
    "    silhouette_avg_kmedoids = silhouette_score(X_pca_reduced, kmedoids.labels_)\n",
    "    silhouette_scores_kmedoids.append(silhouette_avg_kmedoids)\n",
    "\n",
    "# 3. Plot WCSS for K-means and K-medoids\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(K_range, wcss_kmeans, marker='o', linestyle='--', label='K-Means WCSS', color='blue')\n",
    "plt.plot(K_range, wcss_kmedoids, marker='s', linestyle='--', label='K-Medoids WCSS', color='red')\n",
    "plt.xlabel(\"Number of Clusters (k)\")\n",
    "plt.ylabel(\"WCSS (Within-cluster Sum of Squares)\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "# 4. Plot Silhouette Scores for K-means and K-medoids\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(K_range, silhouette_scores_kmeans, marker='o', linestyle='-', label='K-Means Silhouette Score', color='blue')\n",
    "plt.plot(K_range, silhouette_scores_kmedoids, marker='s', linestyle='-', label='K-Medoids Silhouette Score', color='red')\n",
    "plt.xlabel(\"Number of Clusters (k)\")\n",
    "plt.ylabel(\"Silhouette Score\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set number of components for PCA\n",
    "n_components = 3\n",
    "\n",
    "# Reduce dimensionality using PCA\n",
    "pca = PCA(n_components=n_components)\n",
    "X_pca_reduced = pca.fit_transform(X)\n",
    "\n",
    "optimal_k = 20  # Adjust based on the plot\n",
    "\n",
    "kmeans = KMeans(n_clusters=optimal_k, random_state=42, n_init=10)\n",
    "clusters = kmeans.fit_predict(X_pca_reduced)\n",
    "\n",
    "# Assign clusters to the original dataset\n",
    "dfAllStandard[\"Cluster\"] = clusters\n",
    "\n",
    "cluster_distribution = dfAllStandard['Cluster'].value_counts(normalize=True).mul(100).round(2)\n",
    "print(\"Cluster Size (% of Total):\\n\", cluster_distribution)\n",
    "\n",
    "print(dfAllStandard[\"Cluster\"].value_counts())\n",
    "\n",
    "# 5. Analyze clusters by computing feature means per cluster\n",
    "cluster_summary = dfAllStandard.groupby(\"Cluster\")[features].mean()\n",
    "cluster_summary.head(30)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster= 15\n",
    "print(f\"{dfAllStandard[dfAllStandard['Cluster'] == cluster]['KVI_dummy'].sum()}/{dfAllStandard[dfAllStandard['Cluster'] == cluster].shape[0]} = {dfAllStandard[dfAllStandard['Cluster'] == cluster]['KVI_dummy'].sum()/dfAllStandard[dfAllStandard['Cluster'] == cluster].shape[0]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "features = [\n",
    "    'Quantity_Scaled', 'Revenue_Scaled',\n",
    "    'NumCustomers_Scaled', 'PurchaseCount_Scaled', 'DegreeCentrality_Scaled', 'PricePerUnit_Scaled', 'BetweennessCentrality_Scaled'\n",
    "]\n",
    "\n",
    "X = dfAllStandard[features].values  \n",
    "\n",
    "# 1. Apply PCA\n",
    "pca = PCA()\n",
    "X_pca = pca.fit_transform(X)\n",
    "\n",
    "# 2. Explained variance plot (Scree plot)\n",
    "plt.figure(figsize=(8, 5))\n",
    "plt.plot(range(1, len(pca.explained_variance_ratio_) + 1), \n",
    "         np.cumsum(pca.explained_variance_ratio_), \n",
    "         marker='o', linestyle='--')\n",
    "plt.xlabel(\"Number of Principal Components\")\n",
    "plt.ylabel(\"Cumulative Explained Variance\")\n",
    "plt.grid()\n",
    "plt.xticks(range(1, len(pca.explained_variance_ratio_) + 1)) \n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca.explained_variance_ratio_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loadings = pd.DataFrame(pca.components_.T, \n",
    "                        columns=[f'PC{i+1}' for i in range(pca.n_components_)],\n",
    "                        index=features)\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "\n",
    "# Create the heatmap\n",
    "sns.heatmap(loadings[[\"PC1\", \"PC2\", \"PC3\"]], annot=True, cmap='coolwarm', center=0)\n",
    "\n",
    "# Add labels and title\n",
    "plt.xlabel('Principal Component')\n",
    "plt.ylabel('Features')\n",
    "\n",
    "# Show the plot\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loadings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set number of components for PCA\n",
    "n_components = 3\n",
    "\n",
    "# Reduce dimensionality using PCA\n",
    "pca = PCA(n_components=n_components)\n",
    "X_pca_reduced = pca.fit_transform(X)\n",
    "\n",
    "# 1. Apply K-Means Clustering\n",
    "wcss_kmeans = []  # Within-cluster sum of squares for K-Means\n",
    "silhouette_scores_kmeans = []  # Silhouette scores for K-Means\n",
    "K_range = range(2, 60)  # K-means clustering from 2 to 39 clusters\n",
    "\n",
    "for k in K_range:\n",
    "    kmeans = KMeans(n_clusters=k, random_state=42, n_init=10)\n",
    "    kmeans.fit(X_pca_reduced)\n",
    "    wcss_kmeans.append(kmeans.inertia_)  # WCSS for K-means\n",
    "    \n",
    "    # Compute silhouette score for K-means\n",
    "    silhouette_avg_kmeans = silhouette_score(X_pca_reduced, kmeans.labels_)\n",
    "    silhouette_scores_kmeans.append(silhouette_avg_kmeans)\n",
    "\n",
    "# 2. Apply K-Medoids Clustering\n",
    "wcss_kmedoids = []  # Within-cluster sum of squares for K-Medoids\n",
    "silhouette_scores_kmedoids = []  # Silhouette scores for K-Medoids\n",
    "\n",
    "for k in K_range:\n",
    "    kmedoids = KMedoids(n_clusters=k, random_state=42)\n",
    "    kmedoids.fit(X_pca_reduced)\n",
    "    wcss_kmedoids.append(kmedoids.inertia_)  # WCSS for K-medoids\n",
    "    \n",
    "    # Compute silhouette score for K-medoids\n",
    "    silhouette_avg_kmedoids = silhouette_score(X_pca_reduced, kmedoids.labels_)\n",
    "    silhouette_scores_kmedoids.append(silhouette_avg_kmedoids)\n",
    "\n",
    "# 3. Plot WCSS for K-means and K-medoids\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(K_range, wcss_kmeans, marker='o', linestyle='--', label='K-Means WCSS', color='blue')\n",
    "plt.plot(K_range, wcss_kmedoids, marker='s', linestyle='--', label='K-Medoids WCSS', color='red')\n",
    "plt.xlabel(\"Number of Clusters (k)\")\n",
    "plt.ylabel(\"WCSS (Within-cluster Sum of Squares)\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "# 4. Plot Silhouette Scores for K-means and K-medoids\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(K_range, silhouette_scores_kmeans, marker='o', linestyle='-', label='K-Means Silhouette Score', color='blue')\n",
    "plt.plot(K_range, silhouette_scores_kmedoids, marker='s', linestyle='-', label='K-Medoids Silhouette Score', color='red')\n",
    "plt.xlabel(\"Number of Clusters (k)\")\n",
    "plt.ylabel(\"Silhouette Score\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Apply K-Means Clustering (Optimal k selection using Elbow Method)\n",
    "wcss = []  # Within-cluster sum of squares\n",
    "silhouette_scores = []  # Silhouette scores\n",
    "K_range = range(2, 40) \n",
    "\n",
    "for k in K_range:\n",
    "    kmeans = KMeans(n_clusters=k, random_state=42, n_init=10)\n",
    "    kmeans.fit(X_pca_reduced)\n",
    "    wcss.append(kmeans.inertia_)\n",
    "    \n",
    "    # Compute silhouette score\n",
    "    silhouette_avg = silhouette_score(X_pca_reduced, kmeans.labels_)\n",
    "    silhouette_scores.append(silhouette_avg)\n",
    "\n",
    "# Plot the Elbow Method (WCSS)\n",
    "plt.figure(figsize=(12, 5))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(K_range, wcss, marker='o', linestyle='--', label='WCSS')\n",
    "plt.xlabel(\"Number of Clusters (k)\")\n",
    "plt.ylabel(\"WCSS (Within-cluster Sum of Squares)\")\n",
    "plt.title(\"Elbow Method for Optimal k\")\n",
    "plt.grid()\n",
    "plt.legend()\n",
    "\n",
    "# Plot the Silhouette Score\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(K_range, silhouette_scores, marker='s', linestyle='-', color='red', label='Silhouette Score')\n",
    "plt.xlabel(\"Number of Clusters (k)\")\n",
    "plt.ylabel(\"Silhouette Score\")\n",
    "plt.title(\"Silhouette Score for Different k Values\")\n",
    "plt.grid()\n",
    "plt.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimal_k = 20  # Adjust based on the plot\n",
    "\n",
    "kmeans = KMeans(n_clusters=optimal_k, random_state=42, n_init=10)\n",
    "clusters = kmeans.fit_predict(X_pca_reduced)\n",
    "\n",
    "# Assign clusters to the original dataset\n",
    "dfAllStandard[\"Cluster\"] = clusters\n",
    "\n",
    "cluster_distribution = dfAllStandard['Cluster'].value_counts(normalize=True).mul(100).round(2)\n",
    "print(\"Cluster Size (% of Total):\\n\", cluster_distribution)\n",
    "\n",
    "print(dfAllStandard[\"Cluster\"].value_counts())\n",
    "\n",
    "# 5. Analyze clusters by computing feature means per cluster\n",
    "cluster_summary = dfAllStandard.groupby(\"Cluster\")[features].mean()\n",
    "cluster_summary.head(30)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster= 10\n",
    "print(f\"{dfAllStandard[dfAllStandard['Cluster'] == cluster]['KVI_dummy'].sum()}/{dfAllStandard[dfAllStandard['Cluster'] == cluster].shape[0]} = {dfAllStandard[dfAllStandard['Cluster'] == cluster]['KVI_dummy'].sum()/dfAllStandard[dfAllStandard['Cluster'] == cluster].shape[0]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for cluster in range (0, 20):\n",
    "    print(len(dfAllStandard[dfAllStandard['Cluster'] == cluster]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for cluster in range (0, 20):\n",
    "    print(f\"{dfAllStandard[dfAllStandard['Cluster'] == cluster]['KVI_dummy'].sum()}/{dfAllStandard[dfAllStandard['Cluster'] == cluster].shape[0]} = {dfAllStandard[dfAllStandard['Cluster'] == cluster]['KVI_dummy'].sum()/dfAllStandard[dfAllStandard['Cluster'] == cluster].shape[0]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfAllStandard[\"KVI_dummy\"].sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfAllStandard[dfAllStandard[\"KVI_dummy\"] == 1][features].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfAllStandard[dfAllStandard[\"KVI_dummy\"] == 0][features].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfAllStandard[dfAllStandard[\"KVI_dummy\"] == 1][features].sort_values(\"Revenue_Scaled\", ascending=False).head(56).describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfAllStandard[dfAllStandard[\"KVI_dummy\"] == 1].head(56)[\"Cluster\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfAllStandard.head(56)[\"Cluster\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfAllStandard[dfAllStandard[\"KVI_dummy\"] == 1].iloc[56:213][\"Cluster\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Create masks\n",
    "kvi_mask = dfAllStandard[\"KVI_dummy\"] == 1\n",
    "non_kvi_mask = ~kvi_mask\n",
    "cluster_6_mask = dfAllStandard[\"Cluster\"] == 10\n",
    "non_cluster_6_mask = ~cluster_6_mask\n",
    "\n",
    "# Use only first 2 PCA components\n",
    "reduced_data_2d = X_pca_reduced[:, :2]\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10, 7))\n",
    "\n",
    "# Plot non-cluster 6 points (all grey)\n",
    "# Circles for non-KVI\n",
    "ax.scatter(reduced_data_2d[non_cluster_6_mask & non_kvi_mask, 0],\n",
    "           reduced_data_2d[non_cluster_6_mask & non_kvi_mask, 1],\n",
    "           color='lightgrey', alpha=0.6, label='Other - Non-KVI', marker='o')\n",
    "\n",
    "\n",
    "# Plot cluster 6 points (orange)\n",
    "# Circles for non-KVI\n",
    "ax.scatter(reduced_data_2d[cluster_6_mask & non_kvi_mask, 0],\n",
    "           reduced_data_2d[cluster_6_mask & non_kvi_mask, 1],\n",
    "           color='orange', alpha=0.9, label='Cluster 10 - Non-KVI', marker='o')\n",
    "\n",
    "# Triangles for KVI - outside cluster 6 (grey)\n",
    "ax.scatter(reduced_data_2d[non_cluster_6_mask & kvi_mask, 0],\n",
    "           reduced_data_2d[non_cluster_6_mask & kvi_mask, 1],\n",
    "           color='lightgrey', edgecolors='black', linewidths=.5,\n",
    "           alpha=0.6, label='Other - KVI', marker='^')\n",
    "\n",
    "# Triangles for KVI - inside cluster 6 (orange)\n",
    "ax.scatter(reduced_data_2d[cluster_6_mask & kvi_mask, 0],\n",
    "           reduced_data_2d[cluster_6_mask & kvi_mask, 1],\n",
    "           color='orange', edgecolors='black', linewidths=.5,\n",
    "           alpha=0.9, label='Cluster 10 - KVI', marker='^')\n",
    "\n",
    "ax.set_xlabel(\"PCA Component 1\")\n",
    "ax.set_ylabel(\"PCA Component 2\")\n",
    "\n",
    "# Avoid duplicate labels in legend\n",
    "handles, labels = ax.get_legend_handles_labels()\n",
    "by_label = dict(zip(labels, handles))\n",
    "ax.legend(by_label.values(), by_label.keys())\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the total number of KVI_dummy = 1 rows\n",
    "total_kvi = dfAllStandard[dfAllStandard['KVI_dummy'] == 1].shape[0]\n",
    "\n",
    "# Calculate the number of KVI_dummy = 1 rows for each cluster\n",
    "kvi_per_cluster = dfAllStandard[dfAllStandard['KVI_dummy'] == 1]['Cluster'].value_counts()\n",
    "\n",
    "# Calculate the proportion of KVI_dummy = 1 rows for each cluster\n",
    "kvi_proportion_per_cluster = (kvi_per_cluster / total_kvi) * 100\n",
    "\n",
    "# Display the results\n",
    "print(kvi_proportion_per_cluster)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the total number of rows per cluster\n",
    "total_rows_per_cluster = dfAllStandard['Cluster'].value_counts()\n",
    "\n",
    "# Calculate the number of KVI_dummy = 1 rows per cluster\n",
    "kvi_rows_per_cluster = dfAllStandard[dfAllStandard['KVI_dummy'] == 1]['Cluster'].value_counts()\n",
    "\n",
    "# Calculate the accuracy per cluster\n",
    "accuracy_per_cluster = (kvi_rows_per_cluster / total_rows_per_cluster) * 100\n",
    "\n",
    "# Display the results\n",
    "print(accuracy_per_cluster)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster_number = 10\n",
    "KVI_Cluster = dfAllStandard[dfAllStandard[\"Cluster\"] == cluster_number][\"ProductId\"].to_list()\n",
    "\n",
    "\n",
    "invoice_products = OrderLinesData.groupby('InvoiceId')['ProductId'].apply(list)\n",
    "\n",
    "\n",
    "pair_list = []\n",
    "for product_list in invoice_products:\n",
    "    pairs = combinations(sorted(set(product_list)), 2)  # remove duplicates in invoice\n",
    "    pair_list.extend(pairs)\n",
    "\n",
    "pair_df = pd.DataFrame(pair_list, columns=['Product_A', 'Product_B'])\n",
    "\n",
    "pair_counts = pair_df.value_counts().reset_index(name='Count')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pair_counts[\"KVI_A\"] = pair_counts[\"Product_A\"].isin(KVI_Cluster).astype(int)\n",
    "pair_counts[\"KVI_B\"] = pair_counts[\"Product_B\"].isin(KVI_Cluster).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pair_counts = pair_counts[pair_counts[\"KVI_A\"] + pair_counts[\"KVI_B\"] > 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pair_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "OrderLinesData[\"InvoiceCreationDate\"].nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pair_counts = pair_counts[pair_counts[\"Count\"] >= 161]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.concat([\n",
    "    pd.Series(pair_counts[pair_counts[\"KVI_B\"] == 1][\"Product_B\"].unique()),\n",
    "    pd.Series(pair_counts[pair_counts[\"KVI_A\"] == 1][\"Product_A\"].unique())\n",
    "]).nunique()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "mask = (pair_counts[\"KVI_A\"] == 0) & (pair_counts[\"KVI_B\"] == 1)\n",
    "\n",
    "pair_counts.loc[mask, [\"Product_A\", \"Product_B\"]] = pair_counts.loc[mask, [\"Product_B\", \"Product_A\"]].values\n",
    "pair_counts.loc[mask, [\"KVI_A\", \"KVI_B\"]] = pair_counts.loc[mask, [\"KVI_B\", \"KVI_A\"]].values\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pair_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "results = []\n",
    "\n",
    "for _, row in pair_counts.iterrows():\n",
    "    prod_a = row['Product_A']\n",
    "    prod_b = row['Product_B']\n",
    "    \n",
    "    # Get all invoices containing A\n",
    "    invoices_a = OrderLinesData[OrderLinesData['ProductId'] == prod_a][['InvoiceId', 'PricePerUnit']]\n",
    "    invoices_a = invoices_a.rename(columns={'PricePerUnit': 'Price_A'})\n",
    "\n",
    "    # Get all invoices containing B\n",
    "    invoices_b = OrderLinesData[OrderLinesData['ProductId'] == prod_b][['InvoiceId', 'Quantity']]\n",
    "    invoices_b = invoices_b.rename(columns={'Quantity': 'Quantity_B'})\n",
    "    \n",
    "    # Merge on InvoiceId to get only those invoices where both appear\n",
    "    merged = invoices_a.merge(invoices_b, on='InvoiceId')\n",
    "\n",
    "    if len(merged) >= 10:\n",
    "        merged = merged[(merged['Quantity_B'] > 0) & (merged['Price_A'] > 0)]\n",
    "\n",
    "        # Log-transform\n",
    "        merged['log_Q_B'] = np.log(merged['Quantity_B'])\n",
    "        merged['log_P_A'] = np.log(merged['Price_A'])\n",
    "\n",
    "        # Add constant for intercept\n",
    "        X = sm.add_constant(merged['log_P_A'])\n",
    "        y = merged['log_Q_B']\n",
    "\n",
    "        # Fit regression\n",
    "        model = sm.OLS(y, X).fit()\n",
    "\n",
    "        elasticity = model.params['log_P_A']\n",
    "        p_value = model.pvalues['log_P_A']\n",
    "        unique_prices = merged['Price_A'].nunique()\n",
    "        unique_quantities = merged[\"Quantity_B\"].nunique()\n",
    "        \n",
    "        results.append({\n",
    "            'Product_A': prod_a,\n",
    "            'Product_B': prod_b,\n",
    "            'Cross_Elasticity': elasticity,\n",
    "            'P_Value': p_value,\n",
    "            'Num_Samples': len(merged),\n",
    "            'Unique_Prices_A': unique_prices,\n",
    "            'Unique_Quantities_B': unique_quantities\n",
    "        })\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df = pd.DataFrame(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pair_counts = pair_counts.merge(results_df.filter([\"Product_A\", \"Product_B\", \"Cross_Elasticity\", \"P_Value\", \"Unique_Prices_A\", \"Unique_Quantities_B\"]), on=[\"Product_A\", \"Product_B\"], how=\"left\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_price_elasticity(cluster_num, threshold=161):\n",
    "    KVI_Cluster = dfAllStandard[dfAllStandard[\"Cluster\"] == cluster_num][\"ProductId\"].to_list()\n",
    "    invoice_products = OrderLinesData.groupby('InvoiceId')['ProductId'].apply(list)\n",
    "    pair_list = []\n",
    "    for product_list in invoice_products:\n",
    "        pairs = combinations(sorted(set(product_list)), 2)  # remove duplicates in invoice\n",
    "        pair_list.extend(pairs)\n",
    "\n",
    "    pair_df = pd.DataFrame(pair_list, columns=['Product_A', 'Product_B'])\n",
    "\n",
    "    pair_counts = pair_df.value_counts().reset_index(name='Count')\n",
    "    pair_counts[\"KVI_A\"] = pair_counts[\"Product_A\"].isin(KVI_Cluster).astype(int)\n",
    "    pair_counts[\"KVI_B\"] = pair_counts[\"Product_B\"].isin(KVI_Cluster).astype(int)\n",
    "    pair_counts = pair_counts[pair_counts[\"KVI_A\"] + pair_counts[\"KVI_B\"] > 0]\n",
    "    pair_counts = pair_counts[pair_counts[\"Count\"] >= threshold]\n",
    "\n",
    "    if len(pair_counts) == 0:\n",
    "        return \"No pairs\"\n",
    "    pd.concat([\n",
    "        pd.Series(pair_counts[pair_counts[\"KVI_B\"] == 1][\"Product_B\"].unique()),\n",
    "        pd.Series(pair_counts[pair_counts[\"KVI_A\"] == 1][\"Product_A\"].unique())\n",
    "    ]).nunique()\n",
    "    mask = (pair_counts[\"KVI_A\"] == 0) & (pair_counts[\"KVI_B\"] == 1)\n",
    "\n",
    "    pair_counts.loc[mask, [\"Product_A\", \"Product_B\"]] = pair_counts.loc[mask, [\"Product_B\", \"Product_A\"]].values\n",
    "    pair_counts.loc[mask, [\"KVI_A\", \"KVI_B\"]] = pair_counts.loc[mask, [\"KVI_B\", \"KVI_A\"]].values\n",
    "\n",
    "\n",
    "    results = []\n",
    "\n",
    "    for _, row in pair_counts.iterrows():\n",
    "        prod_a = row['Product_A']\n",
    "        prod_b = row['Product_B']\n",
    "        \n",
    "        # Get all invoices containing A\n",
    "        invoices_a = OrderLinesData[OrderLinesData['ProductId'] == prod_a][['InvoiceId', 'PricePerUnit']]\n",
    "        invoices_a = invoices_a.rename(columns={'PricePerUnit': 'Price_A'})\n",
    "\n",
    "        # Get all invoices containing B\n",
    "        invoices_b = OrderLinesData[OrderLinesData['ProductId'] == prod_b][['InvoiceId', 'Quantity']]\n",
    "        invoices_b = invoices_b.rename(columns={'Quantity': 'Quantity_B'})\n",
    "        \n",
    "        # Merge on InvoiceId to get only those invoices where both appear\n",
    "        merged = invoices_a.merge(invoices_b, on='InvoiceId')\n",
    "\n",
    "        if len(merged) >= 10:\n",
    "            merged = merged[(merged['Quantity_B'] > 0) & (merged['Price_A'] > 0)]\n",
    "\n",
    "            # Log-transform\n",
    "            merged['log_Q_B'] = np.log(merged['Quantity_B'])\n",
    "            merged['log_P_A'] = np.log(merged['Price_A'])\n",
    "\n",
    "            # Add constant for intercept\n",
    "            X = sm.add_constant(merged['log_P_A'])\n",
    "            y = merged['log_Q_B']\n",
    "\n",
    "            # Fit regression\n",
    "            # Fit regression with clustered standard errors by InvoiceId\n",
    "            model = sm.OLS(y, X).fit(cov_type='cluster', cov_kwds={'groups': merged['InvoiceId']})\n",
    "\n",
    "\n",
    "            elasticity = model.params['log_P_A']\n",
    "            p_value = model.pvalues['log_P_A']\n",
    "            unique_prices = merged['Price_A'].nunique()\n",
    "            unique_quantities = merged[\"Quantity_B\"].nunique()\n",
    "            \n",
    "            results.append({\n",
    "                'Product_A': prod_a,\n",
    "                'Product_B': prod_b,\n",
    "                'Cross_Elasticity': elasticity,\n",
    "                'P_Value': p_value,\n",
    "                'Num_Samples': len(merged),\n",
    "                'Unique_Prices_A': unique_prices,\n",
    "                'Unique_Quantities_B': unique_quantities\n",
    "            })\n",
    "\n",
    "    results_df = pd.DataFrame(results)\n",
    "    pair_counts = pair_counts.merge(results_df.filter([\"Product_A\", \"Product_B\", \"Cross_Elasticity\", \"P_Value\", \"Unique_Prices_A\", \"Unique_Quantities_B\"]), on=[\"Product_A\", \"Product_B\"], how=\"left\")\n",
    "    # Filter the DataFrame first\n",
    "    filtered = pair_counts[(pair_counts[\"P_Value\"] < 0.05) & (pair_counts[\"Unique_Prices_A\"] > 10)]\n",
    "    if len(filtered) == 0:\n",
    "        return \"No pairs\"\n",
    "\n",
    "    # Compute weighted average of Cross_Elasticity using 'Count' as weights\n",
    "    weighted_avg = np.average(filtered[\"Cross_Elasticity\"], weights=filtered[\"Count\"])\n",
    "\n",
    "    print(f\"Cluster {cluster_num} Weighted Average Cross-Elasticity:\", weighted_avg, \"Average Cross-Elasticity:\", filtered[\"Cross_Elasticity\"].mean(), \"Number of Pairs:\", len(filtered))\n",
    "\n",
    "for n in range(0,20):\n",
    "    cross_price_elasticity(n, 161)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "KVI_Cluster = dfAllStandard[dfAllStandard[\"KVI_dummy\"] == 1][\"ProductId\"].to_list()\n",
    "invoice_products = OrderLinesData.groupby('InvoiceId')['ProductId'].apply(list)\n",
    "pair_list = []\n",
    "for product_list in invoice_products:\n",
    "    pairs = combinations(sorted(set(product_list)), 2)  # remove duplicates in invoice\n",
    "    pair_list.extend(pairs)\n",
    "\n",
    "pair_df = pd.DataFrame(pair_list, columns=['Product_A', 'Product_B'])\n",
    "\n",
    "pair_counts = pair_df.value_counts().reset_index(name='Count')\n",
    "pair_counts[\"KVI_A\"] = pair_counts[\"Product_A\"].isin(KVI_Cluster).astype(int)\n",
    "pair_counts[\"KVI_B\"] = pair_counts[\"Product_B\"].isin(KVI_Cluster).astype(int)\n",
    "pair_counts = pair_counts[pair_counts[\"KVI_A\"] + pair_counts[\"KVI_B\"] > 0]\n",
    "pair_counts = pair_counts[pair_counts[\"Count\"] >= 161]\n",
    "\n",
    "if len(pair_counts) == 0:\n",
    "    print(\"No pairs\")\n",
    "pd.concat([\n",
    "    pd.Series(pair_counts[pair_counts[\"KVI_B\"] == 1][\"Product_B\"].unique()),\n",
    "    pd.Series(pair_counts[pair_counts[\"KVI_A\"] == 1][\"Product_A\"].unique())\n",
    "]).nunique()\n",
    "mask = (pair_counts[\"KVI_A\"] == 0) & (pair_counts[\"KVI_B\"] == 1)\n",
    "\n",
    "pair_counts.loc[mask, [\"Product_A\", \"Product_B\"]] = pair_counts.loc[mask, [\"Product_B\", \"Product_A\"]].values\n",
    "pair_counts.loc[mask, [\"KVI_A\", \"KVI_B\"]] = pair_counts.loc[mask, [\"KVI_B\", \"KVI_A\"]].values\n",
    "\n",
    "\n",
    "results = []\n",
    "\n",
    "for _, row in pair_counts.iterrows():\n",
    "    prod_a = row['Product_A']\n",
    "    prod_b = row['Product_B']\n",
    "    \n",
    "    # Get all invoices containing A\n",
    "    invoices_a = OrderLinesData[OrderLinesData['ProductId'] == prod_a][['InvoiceId', 'PricePerUnit']]\n",
    "    invoices_a = invoices_a.rename(columns={'PricePerUnit': 'Price_A'})\n",
    "\n",
    "    # Get all invoices containing B\n",
    "    invoices_b = OrderLinesData[OrderLinesData['ProductId'] == prod_b][['InvoiceId', 'Quantity']]\n",
    "    invoices_b = invoices_b.rename(columns={'Quantity': 'Quantity_B'})\n",
    "    \n",
    "    # Merge on InvoiceId to get only those invoices where both appear\n",
    "    merged = invoices_a.merge(invoices_b, on='InvoiceId')\n",
    "\n",
    "    if len(merged) >= 10:\n",
    "        merged = merged[(merged['Quantity_B'] > 0) & (merged['Price_A'] > 0)]\n",
    "\n",
    "        # Log-transform\n",
    "        merged['log_Q_B'] = np.log(merged['Quantity_B'])\n",
    "        merged['log_P_A'] = np.log(merged['Price_A'])\n",
    "\n",
    "        # Add constant for intercept\n",
    "        X = sm.add_constant(merged['log_P_A'])\n",
    "        y = merged['log_Q_B']\n",
    "\n",
    "        # Fit regression\n",
    "        model = sm.OLS(y, X).fit(cov_type='cluster', cov_kwds={'groups': merged['InvoiceId']})\n",
    "\n",
    "\n",
    "        elasticity = model.params['log_P_A']\n",
    "        p_value = model.pvalues['log_P_A']\n",
    "        unique_prices = merged['Price_A'].nunique()\n",
    "        unique_quantities = merged[\"Quantity_B\"].nunique()\n",
    "        \n",
    "        results.append({\n",
    "            'Product_A': prod_a,\n",
    "            'Product_B': prod_b,\n",
    "            'Cross_Elasticity': elasticity,\n",
    "            'P_Value': p_value,\n",
    "            'Num_Samples': len(merged),\n",
    "            'Unique_Prices_A': unique_prices,\n",
    "            'Unique_Quantities_B': unique_quantities\n",
    "        })\n",
    "\n",
    "results_df = pd.DataFrame(results)\n",
    "pair_counts = pair_counts.merge(results_df.filter([\"Product_A\", \"Product_B\", \"Cross_Elasticity\", \"P_Value\", \"Unique_Prices_A\", \"Unique_Quantities_B\"]), on=[\"Product_A\", \"Product_B\"], how=\"left\")\n",
    "# Filter the DataFrame first\n",
    "filtered = pair_counts[(pair_counts[\"P_Value\"] < 0.05) & (pair_counts[\"Unique_Prices_A\"] > 10)]\n",
    "\n",
    "# Compute weighted average of Cross_Elasticity using 'Count' as weights\n",
    "weighted_avg = np.average(filtered[\"Cross_Elasticity\"], weights=filtered[\"Count\"])\n",
    "\n",
    "print(f\"Weighted Average Cross-Elasticity:\", weighted_avg, \"Average Cross-Elasticity:\", filtered[\"Cross_Elasticity\"].mean(), \"Number of Pairs:\", len(filtered))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfRegression_t['Quantity'] = np.log(dfRegression_t['Quantity'])\n",
    "dfRegression_t['PricePerUnit'] = np.log(dfRegression_t['PricePerUnit'])\n",
    "dfRegression_t['PriceChangeFrequency'] = np.log(dfRegression_t['PriceChangeFrequency']+1)\n",
    "dfRegression_t['NumCustomers'] = np.log(dfRegression_t['NumCustomers'])\n",
    "\n",
    "\n",
    "formula = 'Quantity ~ PricePerUnit*PriceChangeFrequency + PricePerUnit*C(ProductType) + PricePerUnit*C(Availability) + PricePerUnit*NumCustomers'\n",
    "\n",
    "model_1 = smf.ols(formula=formula, data=dfRegression_t)\n",
    "res_model_1 = model_1.fit(cov_type='cluster', cov_kwds={'groups': dfRegression_t['ProductId']})\n",
    "\n",
    "# Print the summary to check the results\n",
    "print(res_model_1.summary())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "KVI_Cluster = dfAllStandard[dfAllStandard[\"Cluster\"] == 10][\"ProductId\"].to_list()\n",
    "dfRegression_t[\"KVI\"] = dfRegression_t[\"ProductId\"].isin(KVI_Cluster).astype(int)\n",
    "model_2 = smf.ols(formula= 'Quantity ~ PricePerUnit*PriceChangeFrequency +  PricePerUnit*C(ProductType) + PricePerUnit*C(Availability) + PricePerUnit*NumCustomers + PricePerUnit*KVI' ,\n",
    "                data=dfRegression_t)\n",
    "\n",
    "res_2 = model_2.fit(cov_type='cluster', cov_kwds={'groups': dfRegression_t['ProductId']})\n",
    "print(res_2.summary())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "KVI_Cluster = dfAllStandard[dfAllStandard[\"Cluster\"] == 17][\"ProductId\"].to_list()\n",
    "dfRegression_t[\"KVI\"] = dfRegression_t[\"ProductId\"].isin(KVI_Cluster).astype(int)\n",
    "model_2_placebo_17 = smf.ols(formula= 'Quantity ~ PricePerUnit*PriceChangeFrequency +  PricePerUnit*C(ProductType) + PricePerUnit*C(Availability) + PricePerUnit*NumCustomers + PricePerUnit*KVI' ,\n",
    "                data=dfRegression_t)\n",
    "\n",
    "res_2_placebo_17 = model_2_placebo_17.fit(cov_type='cluster', cov_kwds={'groups': dfRegression_t['ProductId']})\n",
    "print(res_2_placebo_17.summary())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "KVI_Cluster = dfAllStandard[dfAllStandard[\"Cluster\"] == 8][\"ProductId\"].to_list()\n",
    "dfRegression_t[\"KVI\"] = dfRegression_t[\"ProductId\"].isin(KVI_Cluster).astype(int)\n",
    "model_2_placebo_8 = smf.ols(formula= 'Quantity ~ PricePerUnit*PriceChangeFrequency +  PricePerUnit*C(ProductType) + PricePerUnit*C(Availability) + PricePerUnit*NumCustomers + PricePerUnit*KVI' ,\n",
    "                data=dfRegression_t)\n",
    "\n",
    "res_2_placebo_8 = model_2_placebo_8.fit(cov_type='cluster', cov_kwds={'groups': dfRegression_t['ProductId']})\n",
    "print(res_2_placebo_8.summary())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "KVI_Cluster = dfAllStandard[(dfAllStandard[\"Cluster\"] == 10) | (dfAllStandard[\"Cluster\"] == 17) ][\"ProductId\"].to_list()\n",
    "dfRegression_t[\"KVI\"] = dfRegression_t[\"ProductId\"].isin(KVI_Cluster).astype(int)\n",
    "model_2_placebo_10_17 = smf.ols(formula= 'Quantity ~ PricePerUnit*PriceChangeFrequency +  PricePerUnit*C(ProductType) + PricePerUnit*C(Availability) + PricePerUnit*NumCustomers + PricePerUnit*KVI' ,\n",
    "                data=dfRegression_t)\n",
    "\n",
    "res_2_placebo_10_17 = model_2_placebo_10_17.fit(cov_type='cluster', cov_kwds={'groups': dfRegression_t['ProductId']})\n",
    "print(res_2_placebo_10_17.summary())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "KVI_Cluster = dfAllStandard[dfAllStandard[\"KVI_dummy\"] == 1][\"ProductId\"].to_list()\n",
    "dfRegression_t[\"KVI\"] = dfRegression_t[\"ProductId\"].isin(KVI_Cluster).astype(int)\n",
    "model_2_placebo_list = smf.ols(formula= 'Quantity ~ PricePerUnit*PriceChangeFrequency +  PricePerUnit*C(ProductType) + PricePerUnit*C(Availability) + PricePerUnit*NumCustomers + PricePerUnit*KVI' ,\n",
    "                data=dfRegression_t)\n",
    "\n",
    "res_2_placebo_list = model_2_placebo_list.fit(cov_type='cluster', cov_kwds={'groups': dfRegression_t['ProductId']})\n",
    "print(res_2_placebo_list.summary())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Compare model_1 (baseline) to model_2\n",
    "anova_results_2 = anova_lm(res_model_1, res_2)\n",
    "print(\"ANOVA: model_1 vs model_2\")\n",
    "print(anova_results_2)\n",
    "\n",
    "# Compare model_1 (baseline) to model_2_placebo_17\n",
    "anova_results_placebo_17 = anova_lm(res_model_1, res_2_placebo_17)\n",
    "print(\"\\nANOVA: model_1 vs model_2_placebo_17\")\n",
    "print(anova_results_placebo_17)\n",
    "\n",
    "# Compare model_1 (baseline) to model_2_placebo_19\n",
    "anova_results_placebo_8 = anova_lm(res_model_1, res_2_placebo_8)\n",
    "print(\"\\nANOVA: model_1 vs model_2_placebo_8\")\n",
    "print(anova_results_placebo_8)\n",
    "\n",
    "\n",
    "anova_results_placebo_10_17 = anova_lm(res_model_1, res_2_placebo_10_17)\n",
    "print(\"\\nANOVA: model_1 vs model_2_placebo_10_17\")\n",
    "print(anova_results_placebo_10_17)\n",
    "\n",
    "\n",
    "anova_results_placebo_list = anova_lm(res_model_1, res_2_placebo_list)\n",
    "print(\"\\nANOVA: model_1 vs model_2_placebo_list\")\n",
    "print(anova_results_placebo_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "OrderLinesData.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "KVI_Cluster = dfAllStandard[dfAllStandard[\"Cluster\"] == 10][\"ProductId\"].to_list()\n",
    "invoices_with_kvi = OrderLinesData[OrderLinesData[\"ProductId\"].isin(KVI_Cluster)][\"InvoiceId\"].unique()\n",
    "invoice_product_counts = OrderLinesData.groupby('InvoiceId')['ProductId'].nunique()\n",
    "invoices_with_2_or_more = invoice_product_counts[invoice_product_counts >= 2].index\n",
    "valid_invoices = set(invoices_with_kvi) & set(invoices_with_2_or_more)\n",
    "filtered_orderlines = OrderLinesData[OrderLinesData['InvoiceId'].isin(valid_invoices)]\n",
    "filtered_orderlines[\"KVI\"] = filtered_orderlines[\"ProductId\"].isin(KVI_Cluster).astype(int)\n",
    "filtered_orderlines = filtered_orderlines.drop(columns=['RegionId', 'InvoiceAttributeA',\n",
    "       'RegionCurrency', 'CustomerName', 'CustomerDescription',\n",
    "       'CustomerAttributeA', 'PricePerUnitVat', 'MarginPerUnit', 'TotalCostPerUnit',\n",
    "       'PurchaseCostPerUnit', 'ShippingCostPerUnit', 'OtherCostPerUnit',\n",
    "       'FeePercentage', 'VatPercentage', 'OrderLineAttributeA',\n",
    "       'OrderLineAttributeB', 'OrderLineAttributeC', 'Deals', 'Date Month'])\n",
    "\n",
    "kvi_prices = filtered_orderlines[filtered_orderlines['ProductId'].isin(KVI_Cluster)].groupby('InvoiceId')['PricePerUnit'].min()\n",
    "\n",
    "filtered_orderlines = filtered_orderlines.merge(kvi_prices, on='InvoiceId', how='left', suffixes=('', '_KVI'))\n",
    "\n",
    "filtered_orderlines[\"KVIPrice\"] = filtered_orderlines[\"PricePerUnit\"].where(filtered_orderlines[\"KVI\"] == 1, filtered_orderlines[\"PricePerUnit_KVI\"])\n",
    "\n",
    "filtered_orderlines[\"KVIPrice\"] = filtered_orderlines[\"KVIPrice\"].fillna(filtered_orderlines[\"PricePerUnit_KVI\"])\n",
    "\n",
    "filtered_orderlines = filtered_orderlines.drop(columns=['PricePerUnit_KVI'])\n",
    "\n",
    "filtered_orderlines = pd.merge(filtered_orderlines, Products.filter(['ProductId','PriceChangeFrequency','ProductType','Availability']), on='ProductId', how='left')\n",
    "filtered_orderlines = pd.merge(filtered_orderlines, num_customers, on='ProductId', how='left')\n",
    "\n",
    "filtered_orderlines = filtered_orderlines.dropna()\n",
    "\n",
    "\n",
    "filtered_orderlines[\"PricePerUnit\"] = np.log(filtered_orderlines[\"PricePerUnit\"])\n",
    "filtered_orderlines[\"KVIPrice\"] = np.log(filtered_orderlines[\"KVIPrice\"])\n",
    "filtered_orderlines[\"PriceChangeFrequency\"] = np.log(filtered_orderlines[\"PriceChangeFrequency\"]+1)\n",
    "filtered_orderlines[\"NumCustomers\"] = np.log(filtered_orderlines[\"NumCustomers\"])\n",
    "filtered_orderlines[\"Quantity\"] = np.log(filtered_orderlines[\"Quantity\"])\n",
    "\n",
    "\n",
    "\n",
    "model_3= smf.ols(formula= 'Quantity ~ PricePerUnit*PriceChangeFrequency +  PricePerUnit*C(ProductType) + PricePerUnit*C(Availability) + PricePerUnit*NumCustomers' ,\n",
    "                data=filtered_orderlines)\n",
    "\n",
    "res_model_3 = model_3.fit(cov_type='cluster', cov_kwds={'groups': filtered_orderlines['InvoiceId']})\n",
    "print(res_model_3.summary())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "KVI_Cluster = dfAllStandard[dfAllStandard[\"Cluster\"] == 10][\"ProductId\"].to_list()\n",
    "invoices_with_kvi = OrderLinesData[OrderLinesData[\"ProductId\"].isin(KVI_Cluster)][\"InvoiceId\"].unique()\n",
    "invoice_product_counts = OrderLinesData.groupby('InvoiceId')['ProductId'].nunique()\n",
    "invoices_with_2_or_more = invoice_product_counts[invoice_product_counts >= 2].index\n",
    "valid_invoices = set(invoices_with_kvi) & set(invoices_with_2_or_more)\n",
    "filtered_orderlines = OrderLinesData[OrderLinesData['InvoiceId'].isin(valid_invoices)]\n",
    "filtered_orderlines[\"KVI\"] = filtered_orderlines[\"ProductId\"].isin(KVI_Cluster).astype(int)\n",
    "filtered_orderlines = filtered_orderlines.drop(columns=['RegionId', 'InvoiceAttributeA',\n",
    "       'RegionCurrency', 'CustomerName', 'CustomerDescription',\n",
    "       'CustomerAttributeA', 'PricePerUnitVat', 'MarginPerUnit', 'TotalCostPerUnit',\n",
    "       'PurchaseCostPerUnit', 'ShippingCostPerUnit', 'OtherCostPerUnit',\n",
    "       'FeePercentage', 'VatPercentage', 'OrderLineAttributeA',\n",
    "       'OrderLineAttributeB', 'OrderLineAttributeC', 'Deals', 'Date Month'])\n",
    "\n",
    "kvi_prices = filtered_orderlines[filtered_orderlines['ProductId'].isin(KVI_Cluster)].groupby('InvoiceId')['PricePerUnit'].max()\n",
    "\n",
    "filtered_orderlines = filtered_orderlines.merge(kvi_prices, on='InvoiceId', how='left', suffixes=('', '_KVI'))\n",
    "\n",
    "filtered_orderlines[\"KVIPrice\"] = filtered_orderlines[\"PricePerUnit\"].where(filtered_orderlines[\"KVI\"] == 1, filtered_orderlines[\"PricePerUnit_KVI\"])\n",
    "\n",
    "filtered_orderlines[\"KVIPrice\"] = filtered_orderlines[\"KVIPrice\"].fillna(filtered_orderlines[\"PricePerUnit_KVI\"])\n",
    "\n",
    "filtered_orderlines = filtered_orderlines.drop(columns=['PricePerUnit_KVI'])\n",
    "\n",
    "filtered_orderlines = pd.merge(filtered_orderlines, Products.filter(['ProductId','PriceChangeFrequency','ProductType','Availability']), on='ProductId', how='left')\n",
    "filtered_orderlines = pd.merge(filtered_orderlines, num_customers, on='ProductId', how='left')\n",
    "\n",
    "filtered_orderlines = filtered_orderlines.dropna()\n",
    "\n",
    "\n",
    "filtered_orderlines[\"PricePerUnit\"] = np.log(filtered_orderlines[\"PricePerUnit\"])\n",
    "filtered_orderlines[\"KVIPrice\"] = np.log(filtered_orderlines[\"KVIPrice\"])\n",
    "filtered_orderlines[\"PriceChangeFrequency\"] = np.log(filtered_orderlines[\"PriceChangeFrequency\"]+1)\n",
    "filtered_orderlines[\"NumCustomers\"] = np.log(filtered_orderlines[\"NumCustomers\"])\n",
    "filtered_orderlines[\"Quantity\"] = np.log(filtered_orderlines[\"Quantity\"])\n",
    "\n",
    "\n",
    "\n",
    "model_4= smf.ols(formula= 'Quantity ~ PricePerUnit*PriceChangeFrequency +  PricePerUnit*C(ProductType) + PricePerUnit*C(Availability) + PricePerUnit*NumCustomers + PricePerUnit*KVIPrice' ,\n",
    "                data=filtered_orderlines)\n",
    "\n",
    "res_model_4 = model_4.fit(cov_type='cluster', cov_kwds={'groups': filtered_orderlines['InvoiceId']})\n",
    "print(res_model_4.summary())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "KVI_Cluster = dfAllStandard[dfAllStandard[\"Cluster\"] == 17][\"ProductId\"].to_list()\n",
    "invoices_with_kvi = OrderLinesData[OrderLinesData[\"ProductId\"].isin(KVI_Cluster)][\"InvoiceId\"].unique()\n",
    "invoice_product_counts = OrderLinesData.groupby('InvoiceId')['ProductId'].nunique()\n",
    "invoices_with_2_or_more = invoice_product_counts[invoice_product_counts >= 2].index\n",
    "valid_invoices = set(invoices_with_kvi) & set(invoices_with_2_or_more)\n",
    "filtered_orderlines = OrderLinesData[OrderLinesData['InvoiceId'].isin(valid_invoices)]\n",
    "filtered_orderlines[\"KVI\"] = filtered_orderlines[\"ProductId\"].isin(KVI_Cluster).astype(int)\n",
    "filtered_orderlines = filtered_orderlines.drop(columns=['RegionId', 'InvoiceAttributeA',\n",
    "       'RegionCurrency', 'CustomerName', 'CustomerDescription',\n",
    "       'CustomerAttributeA', 'PricePerUnitVat', 'MarginPerUnit', 'TotalCostPerUnit',\n",
    "       'PurchaseCostPerUnit', 'ShippingCostPerUnit', 'OtherCostPerUnit',\n",
    "       'FeePercentage', 'VatPercentage', 'OrderLineAttributeA',\n",
    "       'OrderLineAttributeB', 'OrderLineAttributeC', 'Deals', 'Date Month'])\n",
    "\n",
    "kvi_prices = filtered_orderlines[filtered_orderlines['ProductId'].isin(KVI_Cluster)].groupby('InvoiceId')['PricePerUnit'].max()\n",
    "\n",
    "filtered_orderlines = filtered_orderlines.merge(kvi_prices, on='InvoiceId', how='left', suffixes=('', '_KVI'))\n",
    "\n",
    "filtered_orderlines[\"KVIPrice\"] = filtered_orderlines[\"PricePerUnit\"].where(filtered_orderlines[\"KVI\"] == 1, filtered_orderlines[\"PricePerUnit_KVI\"])\n",
    "\n",
    "filtered_orderlines[\"KVIPrice\"] = filtered_orderlines[\"KVIPrice\"].fillna(filtered_orderlines[\"PricePerUnit_KVI\"])\n",
    "\n",
    "filtered_orderlines = filtered_orderlines.drop(columns=['PricePerUnit_KVI'])\n",
    "\n",
    "filtered_orderlines = pd.merge(filtered_orderlines, Products.filter(['ProductId','PriceChangeFrequency','ProductType','Availability']), on='ProductId', how='left')\n",
    "filtered_orderlines = pd.merge(filtered_orderlines, num_customers, on='ProductId', how='left')\n",
    "\n",
    "filtered_orderlines = filtered_orderlines.dropna()\n",
    "\n",
    "\n",
    "filtered_orderlines[\"PricePerUnit\"] = np.log(filtered_orderlines[\"PricePerUnit\"])\n",
    "filtered_orderlines[\"KVIPrice\"] = np.log(filtered_orderlines[\"KVIPrice\"])\n",
    "filtered_orderlines[\"PriceChangeFrequency\"] = np.log(filtered_orderlines[\"PriceChangeFrequency\"]+1)\n",
    "filtered_orderlines[\"NumCustomers\"] = np.log(filtered_orderlines[\"NumCustomers\"])\n",
    "filtered_orderlines[\"Quantity\"] = np.log(filtered_orderlines[\"Quantity\"])\n",
    "\n",
    "\n",
    "\n",
    "model_17= smf.ols(formula= 'Quantity ~ PricePerUnit*PriceChangeFrequency +  PricePerUnit*C(ProductType) + PricePerUnit*C(Availability) + PricePerUnit*NumCustomers + PricePerUnit*KVIPrice' ,\n",
    "                data=filtered_orderlines)\n",
    "\n",
    "res_model_17 = model_17.fit(cov_type='cluster', cov_kwds={'groups': filtered_orderlines['InvoiceId']})\n",
    "print(res_model_17.summary())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "KVI_Cluster = dfAllStandard[dfAllStandard[\"Cluster\"] == 8][\"ProductId\"].to_list()\n",
    "invoices_with_kvi = OrderLinesData[OrderLinesData[\"ProductId\"].isin(KVI_Cluster)][\"InvoiceId\"].unique()\n",
    "invoice_product_counts = OrderLinesData.groupby('InvoiceId')['ProductId'].nunique()\n",
    "invoices_with_2_or_more = invoice_product_counts[invoice_product_counts >= 2].index\n",
    "valid_invoices = set(invoices_with_kvi) & set(invoices_with_2_or_more)\n",
    "filtered_orderlines = OrderLinesData[OrderLinesData['InvoiceId'].isin(valid_invoices)]\n",
    "filtered_orderlines[\"KVI\"] = filtered_orderlines[\"ProductId\"].isin(KVI_Cluster).astype(int)\n",
    "filtered_orderlines = filtered_orderlines.drop(columns=['RegionId', 'InvoiceAttributeA',\n",
    "       'RegionCurrency', 'CustomerName', 'CustomerDescription',\n",
    "       'CustomerAttributeA', 'PricePerUnitVat', 'MarginPerUnit', 'TotalCostPerUnit',\n",
    "       'PurchaseCostPerUnit', 'ShippingCostPerUnit', 'OtherCostPerUnit',\n",
    "       'FeePercentage', 'VatPercentage', 'OrderLineAttributeA',\n",
    "       'OrderLineAttributeB', 'OrderLineAttributeC', 'Deals', 'Date Month'])\n",
    "\n",
    "kvi_prices = filtered_orderlines[filtered_orderlines['ProductId'].isin(KVI_Cluster)].groupby('InvoiceId')['PricePerUnit'].max()\n",
    "\n",
    "filtered_orderlines = filtered_orderlines.merge(kvi_prices, on='InvoiceId', how='left', suffixes=('', '_KVI'))\n",
    "\n",
    "filtered_orderlines[\"KVIPrice\"] = filtered_orderlines[\"PricePerUnit\"].where(filtered_orderlines[\"KVI\"] == 1, filtered_orderlines[\"PricePerUnit_KVI\"])\n",
    "\n",
    "filtered_orderlines[\"KVIPrice\"] = filtered_orderlines[\"KVIPrice\"].fillna(filtered_orderlines[\"PricePerUnit_KVI\"])\n",
    "\n",
    "filtered_orderlines = filtered_orderlines.drop(columns=['PricePerUnit_KVI'])\n",
    "\n",
    "filtered_orderlines = pd.merge(filtered_orderlines, Products.filter(['ProductId','PriceChangeFrequency','ProductType','Availability']), on='ProductId', how='left')\n",
    "filtered_orderlines = pd.merge(filtered_orderlines, num_customers, on='ProductId', how='left')\n",
    "\n",
    "filtered_orderlines = filtered_orderlines.dropna()\n",
    "\n",
    "\n",
    "filtered_orderlines[\"PricePerUnit\"] = np.log(filtered_orderlines[\"PricePerUnit\"])\n",
    "filtered_orderlines[\"KVIPrice\"] = np.log(filtered_orderlines[\"KVIPrice\"])\n",
    "filtered_orderlines[\"PriceChangeFrequency\"] = np.log(filtered_orderlines[\"PriceChangeFrequency\"]+1)\n",
    "filtered_orderlines[\"NumCustomers\"] = np.log(filtered_orderlines[\"NumCustomers\"])\n",
    "filtered_orderlines[\"Quantity\"] = np.log(filtered_orderlines[\"Quantity\"])\n",
    "\n",
    "\n",
    "\n",
    "model_8= smf.ols(formula= 'Quantity ~ PricePerUnit*PriceChangeFrequency +  PricePerUnit*C(ProductType) + PricePerUnit*C(Availability) + PricePerUnit*NumCustomers + PricePerUnit*KVIPrice' ,\n",
    "                data=filtered_orderlines)\n",
    "\n",
    "res_model_8 = model_8.fit(cov_type='cluster', cov_kwds={'groups': filtered_orderlines['InvoiceId']})\n",
    "print(res_model_8.summary())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "KVI_Cluster = dfAllStandard[(dfAllStandard[\"Cluster\"] == 10) | (dfAllStandard[\"Cluster\"] == 17)][\"ProductId\"].to_list()\n",
    "invoices_with_kvi = OrderLinesData[OrderLinesData[\"ProductId\"].isin(KVI_Cluster)][\"InvoiceId\"].unique()\n",
    "invoice_product_counts = OrderLinesData.groupby('InvoiceId')['ProductId'].nunique()\n",
    "invoices_with_2_or_more = invoice_product_counts[invoice_product_counts >= 2].index\n",
    "valid_invoices = set(invoices_with_kvi) & set(invoices_with_2_or_more)\n",
    "filtered_orderlines = OrderLinesData[OrderLinesData['InvoiceId'].isin(valid_invoices)]\n",
    "filtered_orderlines[\"KVI\"] = filtered_orderlines[\"ProductId\"].isin(KVI_Cluster).astype(int)\n",
    "filtered_orderlines = filtered_orderlines.drop(columns=['RegionId', 'InvoiceAttributeA',\n",
    "       'RegionCurrency', 'CustomerName', 'CustomerDescription',\n",
    "       'CustomerAttributeA', 'PricePerUnitVat', 'MarginPerUnit', 'TotalCostPerUnit',\n",
    "       'PurchaseCostPerUnit', 'ShippingCostPerUnit', 'OtherCostPerUnit',\n",
    "       'FeePercentage', 'VatPercentage', 'OrderLineAttributeA',\n",
    "       'OrderLineAttributeB', 'OrderLineAttributeC', 'Deals', 'Date Month'])\n",
    "\n",
    "kvi_prices = filtered_orderlines[filtered_orderlines['ProductId'].isin(KVI_Cluster)].groupby('InvoiceId')['PricePerUnit'].max()\n",
    "\n",
    "filtered_orderlines = filtered_orderlines.merge(kvi_prices, on='InvoiceId', how='left', suffixes=('', '_KVI'))\n",
    "\n",
    "filtered_orderlines[\"KVIPrice\"] = filtered_orderlines[\"PricePerUnit\"].where(filtered_orderlines[\"KVI\"] == 1, filtered_orderlines[\"PricePerUnit_KVI\"])\n",
    "\n",
    "filtered_orderlines[\"KVIPrice\"] = filtered_orderlines[\"KVIPrice\"].fillna(filtered_orderlines[\"PricePerUnit_KVI\"])\n",
    "\n",
    "filtered_orderlines = filtered_orderlines.drop(columns=['PricePerUnit_KVI'])\n",
    "\n",
    "filtered_orderlines = pd.merge(filtered_orderlines, Products.filter(['ProductId','PriceChangeFrequency','ProductType','Availability']), on='ProductId', how='left')\n",
    "filtered_orderlines = pd.merge(filtered_orderlines, num_customers, on='ProductId', how='left')\n",
    "\n",
    "filtered_orderlines = filtered_orderlines.dropna()\n",
    "\n",
    "\n",
    "filtered_orderlines[\"PricePerUnit\"] = np.log(filtered_orderlines[\"PricePerUnit\"])\n",
    "filtered_orderlines[\"KVIPrice\"] = np.log(filtered_orderlines[\"KVIPrice\"])\n",
    "filtered_orderlines[\"PriceChangeFrequency\"] = np.log(filtered_orderlines[\"PriceChangeFrequency\"]+1)\n",
    "filtered_orderlines[\"NumCustomers\"] = np.log(filtered_orderlines[\"NumCustomers\"])\n",
    "filtered_orderlines[\"Quantity\"] = np.log(filtered_orderlines[\"Quantity\"])\n",
    "\n",
    "\n",
    "\n",
    "model_10_17= smf.ols(formula= 'Quantity ~ PricePerUnit*PriceChangeFrequency +  PricePerUnit*C(ProductType) + PricePerUnit*C(Availability) + PricePerUnit*NumCustomers + PricePerUnit*KVIPrice' ,\n",
    "                data=filtered_orderlines)\n",
    "\n",
    "res_model_10_17 = model_10_17.fit(cov_type='cluster', cov_kwds={'groups': filtered_orderlines['InvoiceId']})\n",
    "print(res_model_10_17.summary())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "invoices_with_kvi = OrderLinesData[OrderLinesData[\"ProductId\"].isin(KVI_list)][\"InvoiceId\"].unique()\n",
    "invoice_product_counts = OrderLinesData.groupby('InvoiceId')['ProductId'].nunique()\n",
    "invoices_with_2_or_more = invoice_product_counts[invoice_product_counts >= 2].index\n",
    "valid_invoices = set(invoices_with_kvi) & set(invoices_with_2_or_more)\n",
    "filtered_orderlines = OrderLinesData[OrderLinesData['InvoiceId'].isin(valid_invoices)]\n",
    "filtered_orderlines[\"KVI\"] = filtered_orderlines[\"ProductId\"].isin(KVI_Cluster).astype(int)\n",
    "filtered_orderlines = filtered_orderlines.drop(columns=['RegionId', 'InvoiceAttributeA',\n",
    "       'RegionCurrency', 'CustomerName', 'CustomerDescription',\n",
    "       'CustomerAttributeA', 'PricePerUnitVat', 'MarginPerUnit', 'TotalCostPerUnit',\n",
    "       'PurchaseCostPerUnit', 'ShippingCostPerUnit', 'OtherCostPerUnit',\n",
    "       'FeePercentage', 'VatPercentage', 'OrderLineAttributeA',\n",
    "       'OrderLineAttributeB', 'OrderLineAttributeC', 'Deals', 'Date Month'])\n",
    "\n",
    "kvi_prices = filtered_orderlines[filtered_orderlines['ProductId'].isin(KVI_Cluster)].groupby('InvoiceId')['PricePerUnit'].max()\n",
    "\n",
    "filtered_orderlines = filtered_orderlines.merge(kvi_prices, on='InvoiceId', how='left', suffixes=('', '_KVI'))\n",
    "\n",
    "filtered_orderlines[\"KVIPrice\"] = filtered_orderlines[\"PricePerUnit\"].where(filtered_orderlines[\"KVI\"] == 1, filtered_orderlines[\"PricePerUnit_KVI\"])\n",
    "\n",
    "filtered_orderlines[\"KVIPrice\"] = filtered_orderlines[\"KVIPrice\"].fillna(filtered_orderlines[\"PricePerUnit_KVI\"])\n",
    "\n",
    "filtered_orderlines = filtered_orderlines.drop(columns=['PricePerUnit_KVI'])\n",
    "\n",
    "filtered_orderlines = pd.merge(filtered_orderlines, Products.filter(['ProductId','PriceChangeFrequency','ProductType','Availability']), on='ProductId', how='left')\n",
    "filtered_orderlines = pd.merge(filtered_orderlines, num_customers, on='ProductId', how='left')\n",
    "\n",
    "filtered_orderlines = filtered_orderlines.dropna()\n",
    "\n",
    "\n",
    "filtered_orderlines[\"PricePerUnit\"] = np.log(filtered_orderlines[\"PricePerUnit\"])\n",
    "filtered_orderlines[\"KVIPrice\"] = np.log(filtered_orderlines[\"KVIPrice\"])\n",
    "filtered_orderlines[\"PriceChangeFrequency\"] = np.log(filtered_orderlines[\"PriceChangeFrequency\"]+1)\n",
    "filtered_orderlines[\"NumCustomers\"] = np.log(filtered_orderlines[\"NumCustomers\"])\n",
    "filtered_orderlines[\"Quantity\"] = np.log(filtered_orderlines[\"Quantity\"])\n",
    "\n",
    "\n",
    "\n",
    "model_list= smf.ols(formula= 'Quantity ~ PricePerUnit*PriceChangeFrequency +  PricePerUnit*C(ProductType) + PricePerUnit*C(Availability) + PricePerUnit*NumCustomers + PricePerUnit*KVIPrice' ,\n",
    "                data=filtered_orderlines)\n",
    "\n",
    "res_model_list = model_list.fit(cov_type='cluster', cov_kwds={'groups': filtered_orderlines['InvoiceId']})\n",
    "print(res_model_list.summary())\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
